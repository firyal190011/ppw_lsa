{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyqcdmzAN9BB"
   },
   "source": [
    "# UAS PPW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling data\n",
    "Crawlling merupakan proses pengambilan data yang tersedia secara online untuk umum. Proses pengambilan data informasi pada halaman menggunakan URL (Uniform Resource Locator). URL ini akan menjadi acuan untuk mencari semua hyperlink yang ada pada website. Kemudian dilakukan indexing untuk mencari kata dalam dokumen pada setiap link yang ada. Lalu jika, menyertakan API (Application Programming Interface) dapat melakukan penambangan dataset yang lebih besar. Selain itu, dengan API kita dapat mengumpulkan data lebih spesifik sesuai dengan link URL yang ada tanpa harus mengetahui element HTML pada sebuah website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalasi Scrapy\n",
    "Untuk menginstall scrapy dapat menggnakan perintah pip di Command Prompt berikut:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (22.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (1.6.2)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (0.6.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (22.4.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (37.0.2)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (4.9.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: tldextract in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (3.3.0)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (21.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (5.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (58.1.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scrapy) (0.2.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.15.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: six>=1.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from parsel>=1.5.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (21.4.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (4.2.0)\n",
      "Requirement already satisfied: twisted-iocpsupport<2,>=1.0.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (1.0.2)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: Automat>=0.8.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: incremental>=21.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (21.3.0)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->scrapy) (3.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->scrapy) (1.5.1)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->scrapy) (2.28.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tldextract->scrapy) (3.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Membuat File Scrapy\n",
    "Setelah selesai menginstall Scrapy, maka buat file scrapy baru dengan kode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2847202481.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    scrapy startproject namaproject\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scrapy startproject namaproject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menjalankan Spyder baru\n",
    "Yang harus dilakukan, masuk ke dalam projectscrapy kemudian membuat spider baru dengan kode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd nama project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setelah menjalankan spyder baru, tuliskan command yang isinya nama file dan alamat website yang akan diambil datanya agar lebih mudah saat akan membuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy genspider example example.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menulis Program Scapper\n",
    "Setelah itu, pada pembuatan spyder file yang tadi telah dibuat telah tersedia dengan nama file yang telah dibuat.\n",
    "\n",
    "Pada file yang telah dibuat akan ada code untuk melakukan scraping yang dapat diubah sesuai keinginan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/080211100070',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/090211200001',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/080211100050',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/100211200002',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/080211100044',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/080211100119',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/080211100103',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/080211100098',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/090211100079',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/090211100089',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/090211100013',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/090211100020',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/090211100064',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/090211100064',\n",
    "            'https://pta.trunojoyo.ac.id/welcome/detail/090211100018'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        # print(response.url)\n",
    "        yield {\n",
    "            'judul': response.css('#content_journal > ul > li > div:nth-child(2) > a::text').extract(),\n",
    "            'penulis': response.css('#content_journal > ul > li > div:nth-child(2) > div:nth-child(2) > span::text').extract(),\n",
    "            'dosen_pembimbing_1': response.css('#content_journal > ul > li > div:nth-child(2) > div:nth-child(3) > span::text').extract(),\n",
    "            'dosen_pembimbing_2': response.css('#content_journal > ul > li > div:nth-child(2) > div:nth-child(4) > span::text').extract(),\n",
    "            'abstrak': response.css('#content_journal > ul > li > div:nth-child(4) > div:nth-child(2) > p::text').extract(),\n",
    "        }\n",
    "        # content_journal > ul > li:nth-child(1) > div:nth-child(1) > a\n",
    "        # content_journal > ul > li:nth-child(1) > div:nth-child(1) > a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menjalankan File Spider\n",
    "Pertama, masuk kedalam direktori spider dulu. setelah itu, jalankan spider dengan command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy runspider namafile.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menyimpan data kedalam csv\n",
    "Data yang telah diambil dari suatu web dapat disimpan dengan kode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl namafile -O namafileyangdiinginkan.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "Preprocessing data adalah tahapan perbersihan data dari suatu kalimat atau kata, link, atau hal –hal yang tidak diperlukan untuk melakukan analisis sentiment. Dataset yang diperoleh dari proses crawling masih berbentuk kata atau kalimat yang tidak beraturan atau tidak terstruktur, sehingga membutuhkan proses yang disebut preprocessing data agar menghasilkan data yang bersih supaya mempermudah proses analisis. Lalu pada Preprocessing sendiri melibatkan validasi dan imputasi data. Tujuan dari validasi adalah untuk menilai tingkat kelengkapan dan akurasi data yang tersaring. Preprocessing data sangat penting karena kesalahan, redundan, missing value, dan data yang tidak konsisten menyebabkan berkurangnya akurasi hasil analisis. Jadi, sebelum mengolah data, kita harus memastikan bahwa data yang akan kita gunakan merupakan data \"bersih\". Ada beberapa cara yang bisa digunakan untuk membersihkan data, tergantung dari jenis masalah yang ada dalam kumpulan data. Adapun tahapan preprocessing data terdiri dari Cleaning data, Case Folding, Tokenizing, Stopword, dan Stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installasi library\n",
    "lakukan installasi library yang diperlukan, jika telah terinstall maka langsung panggil saja "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmqlocY629Rc",
    "outputId": "87968a84-ed18-4537-9f5c-dcdf02029c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\anaconda3\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\asus\\anaconda3\\lib\\site-packages (from nltk) (3.4.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install numpy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifLraNvz27zO",
    "outputId": "b5f0b16b-2a09-4623-b031-9aa121d8c318"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mjCnWD1OQBT"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library yang digunakan\n",
    "memanggil library-library yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "syBvB9Yu25aE"
   },
   "outputs": [],
   "source": [
    "# data visualisation and manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#configure\n",
    "# sets matplotlib to inline and displays graphs below the corressponding cell.\n",
    "%matplotlib inline  \n",
    "style.use('fivethirtyeight')\n",
    "sns.set(style='whitegrid',color_codes=True)\n",
    "\n",
    "#import nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "#preprocessing\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk import word_tokenize,sent_tokenize # tokenizing\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer  # using the Porter Stemmer and Lancaster Stemmer and others\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # lammatizer from WordNet\n",
    "\n",
    "# for named entity recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# vectorizers for creating the document-term-matrix (DTM)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "#stop-words\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCPp0zRCPJij"
   },
   "source": [
    "### Data set\n",
    "berikut data yang digunakan pada program saat ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "85fPkb8C25aJ"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('scrapylsa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "znZAxxNt25aK",
    "outputId": "5a0b4e81-e6c7-4756-fee0-a748d99c7a6f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul</th>\n",
       "      <th>penulis</th>\n",
       "      <th>dosen_pembimbing_1</th>\n",
       "      <th>dosen_pembimbing_2</th>\n",
       "      <th>abstrak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fenomenologi Relawan Kelompok Dukungan Sebaya ...</td>\n",
       "      <td>Penulis : Rina Astaria Mendrova</td>\n",
       "      <td>Dosen Pembimbing I : Dr. Yuliana Rakhmawati, S...</td>\n",
       "      <td>Dosen Pembimbing II :-</td>\n",
       "      <td>Abstrak - Tujuan dari penelitian ini adalah me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KLASIFIKASI KOMPLEKSITAS VISUAL CITRA SAMPAH M...</td>\n",
       "      <td>Penulis : Afni Sakinah</td>\n",
       "      <td>Dosen Pembimbing I : Dr. Indah Agustien Siradj...</td>\n",
       "      <td>Dosen Pembimbing II :Moch. Kautsar Sophan, S.K...</td>\n",
       "      <td>Klasifikasi citra merupakan proses pengelompok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PENENTUAN TINGKAT PRIORITAS PERBAIKAN MODE KEG...</td>\n",
       "      <td>Penulis : Lilis Kurnia Ikamawati</td>\n",
       "      <td>Dosen Pembimbing I : Dr. Kukuh Winarso, S.Si.,...</td>\n",
       "      <td>Dosen Pembimbing II :Issa Dyah Utami, S.T., M.T.</td>\n",
       "      <td>Penelitian ini berlokasikan di PT Semen Indone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analisis Cost Volume Profit Untuk Menentukan T...</td>\n",
       "      <td>Penulis : Husnul Hotimah</td>\n",
       "      <td>Dosen Pembimbing I : Hj. Evaliati Amaniyah, S....</td>\n",
       "      <td>Dosen Pembimbing II :</td>\n",
       "      <td>ABSTRAK\\nPenelitian ini bertujuan untuk menget...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUBUNGAN ANTARA PROKRASTINASI AKADEMIK DENGAN ...</td>\n",
       "      <td>Penulis : ALIFIYA RIZKI LAILY</td>\n",
       "      <td>Dosen Pembimbing I : Fandi Rosi Sarwo Edi, S.K...</td>\n",
       "      <td>Dosen Pembimbing II :</td>\n",
       "      <td>Penelitian ini bertujuan untuk mengetahui hubu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PENGARUH KUALITAS PRODUK DAN HARGA TERHADAP KE...</td>\n",
       "      <td>Penulis : Ma'ruf Sya'roni</td>\n",
       "      <td>Dosen Pembimbing I : Dzikrulloh, S.EI., M.SEI</td>\n",
       "      <td>Dosen Pembimbing II :</td>\n",
       "      <td>ABSTRAK\\nTujuan penelitian ini adalah untuk me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Analisis Tingkat Literasi Zakat Masyarakat di ...</td>\n",
       "      <td>Penulis : Siti Amina</td>\n",
       "      <td>Dosen Pembimbing I : Dr. Abdur Rahman, S.Ag., ...</td>\n",
       "      <td>Dosen Pembimbing II :</td>\n",
       "      <td>Literasi zakat merupakan pengetahuan, keteramp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jaringan Sosial Pengusaha Kerajinan Limbah Kay...</td>\n",
       "      <td>Penulis : ANA FEBRIANTI</td>\n",
       "      <td>Dosen Pembimbing I : Khoirul Rosyadi, Ph.D.</td>\n",
       "      <td>Dosen Pembimbing II :</td>\n",
       "      <td>Ana Febrianti, 160521100004 Program Studi Sosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pengaruh Asset Growth, Current Ratio, dan Debt...</td>\n",
       "      <td>Penulis : Fajar Dianti Khoeroh Saadah</td>\n",
       "      <td>Dosen Pembimbing I : Dahruji, S.E., M.E.I</td>\n",
       "      <td>Dosen Pembimbing II :</td>\n",
       "      <td>Nilai perusahaan merupakan bagian terpenting y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pertukaran Sosial Dalam Tradisi Sape Tok-tok</td>\n",
       "      <td>Penulis : FARIDA YULISTIANA AJI</td>\n",
       "      <td>Dosen Pembimbing I : Dr. Mutmainnah, S.Sos., M.Si</td>\n",
       "      <td>Dosen Pembimbing II :Yudi Rachman, S.Sos., M.S...</td>\n",
       "      <td>Abstrak – Farida Yulistiana Aji. \\n16052110000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               judul  \\\n",
       "0  Fenomenologi Relawan Kelompok Dukungan Sebaya ...   \n",
       "1  KLASIFIKASI KOMPLEKSITAS VISUAL CITRA SAMPAH M...   \n",
       "2  PENENTUAN TINGKAT PRIORITAS PERBAIKAN MODE KEG...   \n",
       "3  Analisis Cost Volume Profit Untuk Menentukan T...   \n",
       "4  HUBUNGAN ANTARA PROKRASTINASI AKADEMIK DENGAN ...   \n",
       "5  PENGARUH KUALITAS PRODUK DAN HARGA TERHADAP KE...   \n",
       "6  Analisis Tingkat Literasi Zakat Masyarakat di ...   \n",
       "7  Jaringan Sosial Pengusaha Kerajinan Limbah Kay...   \n",
       "8  Pengaruh Asset Growth, Current Ratio, dan Debt...   \n",
       "9       Pertukaran Sosial Dalam Tradisi Sape Tok-tok   \n",
       "\n",
       "                                 penulis  \\\n",
       "0        Penulis : Rina Astaria Mendrova   \n",
       "1                 Penulis : Afni Sakinah   \n",
       "2       Penulis : Lilis Kurnia Ikamawati   \n",
       "3               Penulis : Husnul Hotimah   \n",
       "4         Penulis : ALIFIYA RIZKI LAILY    \n",
       "5              Penulis : Ma'ruf Sya'roni   \n",
       "6                   Penulis : Siti Amina   \n",
       "7                Penulis : ANA FEBRIANTI   \n",
       "8  Penulis : Fajar Dianti Khoeroh Saadah   \n",
       "9        Penulis : FARIDA YULISTIANA AJI   \n",
       "\n",
       "                                  dosen_pembimbing_1  \\\n",
       "0  Dosen Pembimbing I : Dr. Yuliana Rakhmawati, S...   \n",
       "1  Dosen Pembimbing I : Dr. Indah Agustien Siradj...   \n",
       "2  Dosen Pembimbing I : Dr. Kukuh Winarso, S.Si.,...   \n",
       "3  Dosen Pembimbing I : Hj. Evaliati Amaniyah, S....   \n",
       "4  Dosen Pembimbing I : Fandi Rosi Sarwo Edi, S.K...   \n",
       "5      Dosen Pembimbing I : Dzikrulloh, S.EI., M.SEI   \n",
       "6  Dosen Pembimbing I : Dr. Abdur Rahman, S.Ag., ...   \n",
       "7        Dosen Pembimbing I : Khoirul Rosyadi, Ph.D.   \n",
       "8          Dosen Pembimbing I : Dahruji, S.E., M.E.I   \n",
       "9  Dosen Pembimbing I : Dr. Mutmainnah, S.Sos., M.Si   \n",
       "\n",
       "                                  dosen_pembimbing_2  \\\n",
       "0                             Dosen Pembimbing II :-   \n",
       "1  Dosen Pembimbing II :Moch. Kautsar Sophan, S.K...   \n",
       "2   Dosen Pembimbing II :Issa Dyah Utami, S.T., M.T.   \n",
       "3                              Dosen Pembimbing II :   \n",
       "4                              Dosen Pembimbing II :   \n",
       "5                              Dosen Pembimbing II :   \n",
       "6                              Dosen Pembimbing II :   \n",
       "7                              Dosen Pembimbing II :   \n",
       "8                              Dosen Pembimbing II :   \n",
       "9  Dosen Pembimbing II :Yudi Rachman, S.Sos., M.S...   \n",
       "\n",
       "                                             abstrak  \n",
       "0  Abstrak - Tujuan dari penelitian ini adalah me...  \n",
       "1  Klasifikasi citra merupakan proses pengelompok...  \n",
       "2  Penelitian ini berlokasikan di PT Semen Indone...  \n",
       "3  ABSTRAK\\nPenelitian ini bertujuan untuk menget...  \n",
       "4  Penelitian ini bertujuan untuk mengetahui hubu...  \n",
       "5  ABSTRAK\\nTujuan penelitian ini adalah untuk me...  \n",
       "6  Literasi zakat merupakan pengetahuan, keteramp...  \n",
       "7  Ana Febrianti, 160521100004 Program Studi Sosi...  \n",
       "8  Nilai perusahaan merupakan bagian terpenting y...  \n",
       "9  Abstrak – Farida Yulistiana Aji. \\n16052110000...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop data yang tidak digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['judul','penulis', 'dosen_pembimbing_1', 'dosen_pembimbing_2'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstrak - Tujuan dari penelitian ini adalah me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Klasifikasi citra merupakan proses pengelompok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Penelitian ini berlokasikan di PT Semen Indone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABSTRAK\\nPenelitian ini bertujuan untuk menget...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Penelitian ini bertujuan untuk mengetahui hubu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ABSTRAK\\nTujuan penelitian ini adalah untuk me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Literasi zakat merupakan pengetahuan, keteramp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ana Febrianti, 160521100004 Program Studi Sosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nilai perusahaan merupakan bagian terpenting y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Abstrak – Farida Yulistiana Aji. \\n16052110000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rumah sakit merupakan salah satu instansi di b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tujuan penelitian ini adalah untuk mengetahui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gerakan literasi pada umumnya sering mengalami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tujuan dalam Penelitian ini adalah mengetahui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Biochar merupakan padatan yang kaya kandungan ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              abstrak\n",
       "0   Abstrak - Tujuan dari penelitian ini adalah me...\n",
       "1   Klasifikasi citra merupakan proses pengelompok...\n",
       "2   Penelitian ini berlokasikan di PT Semen Indone...\n",
       "3   ABSTRAK\\nPenelitian ini bertujuan untuk menget...\n",
       "4   Penelitian ini bertujuan untuk mengetahui hubu...\n",
       "5   ABSTRAK\\nTujuan penelitian ini adalah untuk me...\n",
       "6   Literasi zakat merupakan pengetahuan, keteramp...\n",
       "7   Ana Febrianti, 160521100004 Program Studi Sosi...\n",
       "8   Nilai perusahaan merupakan bagian terpenting y...\n",
       "9   Abstrak – Farida Yulistiana Aji. \\n16052110000...\n",
       "10  Rumah sakit merupakan salah satu instansi di b...\n",
       "11  Tujuan penelitian ini adalah untuk mengetahui ...\n",
       "12  Gerakan literasi pada umumnya sering mengalami...\n",
       "13  Tujuan dalam Penelitian ini adalah mengetahui ...\n",
       "14  Biochar merupakan padatan yang kaya kandungan ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove angka\n",
    "pada proses ini penghapusan angka dan tanda-tanda baca yang ada pada data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re\n",
    "\n",
    "def remove(text):\n",
    "    return re.sub(r\"\\d+\",\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak</th>\n",
       "      <th>abstrak_remove</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstrak - Tujuan dari penelitian ini adalah me...</td>\n",
       "      <td>Abstrak - Tujuan dari penelitian ini adalah me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Klasifikasi citra merupakan proses pengelompok...</td>\n",
       "      <td>Klasifikasi citra merupakan proses pengelompok...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Penelitian ini berlokasikan di PT Semen Indone...</td>\n",
       "      <td>Penelitian ini berlokasikan di PT Semen Indone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABSTRAK\\nPenelitian ini bertujuan untuk menget...</td>\n",
       "      <td>ABSTRAK\\nPenelitian ini bertujuan untuk menget...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Penelitian ini bertujuan untuk mengetahui hubu...</td>\n",
       "      <td>Penelitian ini bertujuan untuk mengetahui hubu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             abstrak  \\\n",
       "0  Abstrak - Tujuan dari penelitian ini adalah me...   \n",
       "1  Klasifikasi citra merupakan proses pengelompok...   \n",
       "2  Penelitian ini berlokasikan di PT Semen Indone...   \n",
       "3  ABSTRAK\\nPenelitian ini bertujuan untuk menget...   \n",
       "4  Penelitian ini bertujuan untuk mengetahui hubu...   \n",
       "\n",
       "                                      abstrak_remove  \n",
       "0  Abstrak - Tujuan dari penelitian ini adalah me...  \n",
       "1  Klasifikasi citra merupakan proses pengelompok...  \n",
       "2  Penelitian ini berlokasikan di PT Semen Indone...  \n",
       "3  ABSTRAK\\nPenelitian ini bertujuan untuk menget...  \n",
       "4  Penelitian ini bertujuan untuk mengetahui hubu...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstrak_remove']=df['abstrak'].apply(remove)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QI8WgVB-OhVS"
   },
   "source": [
    "### Cleansing data\n",
    "\n",
    "  Data cleansing atau yang disebut juga dengan data scrubbing merupakan suatu proses analisa mengenai kualitas dari data dengan mengubah. Bisa juga pengelola mengoreksi ataupun menghapus data tersebut. Data yang dibersihkan tersebut adalah data yang salah, rusak, tidak akurat, tidak lengkap dan salah format. Selain itu, Cleaning data adalah proses menghilangkan atau  menghapus  data  dari  suatu  kalimat  yang  memiliki unsur, seperti username (@username), hastag (#), URL, angka, kalimat yang redundance,emoticon (:@, :D ), dan tanda baca  yang tidak diperlukan pada saat proses analisis sentiment. Pada proses cleaningini pun juga terjadi proses perubahan data ke lowercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e16i7RB725aL"
   },
   "outputs": [],
   "source": [
    "def clean_text(headline):\n",
    "  le=WordNetLemmatizer()\n",
    "  word_tokens=word_tokenize(headline)\n",
    "  tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
    "  cleaned_text=\" \".join(tokens)\n",
    "  return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "byxRz01m25aM"
   },
   "outputs": [],
   "source": [
    "# time taking\n",
    "df['abstrak_cleaned']=df['abstrak_remove'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "5zvhHB9e25aN",
    "outputId": "89e7d6df-dc5a-478e-9c3a-dcaefa21035b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak</th>\n",
       "      <th>abstrak_remove</th>\n",
       "      <th>abstrak_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstrak - Tujuan dari penelitian ini adalah me...</td>\n",
       "      <td>Abstrak - Tujuan dari penelitian ini adalah me...</td>\n",
       "      <td>Abstrak Tujuan dari penelitian adalah mengetah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Klasifikasi citra merupakan proses pengelompok...</td>\n",
       "      <td>Klasifikasi citra merupakan proses pengelompok...</td>\n",
       "      <td>Klasifikasi citra merupakan prose pengelompoka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Penelitian ini berlokasikan di PT Semen Indone...</td>\n",
       "      <td>Penelitian ini berlokasikan di PT Semen Indone...</td>\n",
       "      <td>Penelitian berlokasikan Semen Indonesia Pabrik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABSTRAK\\nPenelitian ini bertujuan untuk menget...</td>\n",
       "      <td>ABSTRAK\\nPenelitian ini bertujuan untuk menget...</td>\n",
       "      <td>ABSTRAK Penelitian bertujuan untuk mengetahui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Penelitian ini bertujuan untuk mengetahui hubu...</td>\n",
       "      <td>Penelitian ini bertujuan untuk mengetahui hubu...</td>\n",
       "      <td>Penelitian bertujuan untuk mengetahui hubungan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             abstrak  \\\n",
       "0  Abstrak - Tujuan dari penelitian ini adalah me...   \n",
       "1  Klasifikasi citra merupakan proses pengelompok...   \n",
       "2  Penelitian ini berlokasikan di PT Semen Indone...   \n",
       "3  ABSTRAK\\nPenelitian ini bertujuan untuk menget...   \n",
       "4  Penelitian ini bertujuan untuk mengetahui hubu...   \n",
       "\n",
       "                                      abstrak_remove  \\\n",
       "0  Abstrak - Tujuan dari penelitian ini adalah me...   \n",
       "1  Klasifikasi citra merupakan proses pengelompok...   \n",
       "2  Penelitian ini berlokasikan di PT Semen Indone...   \n",
       "3  ABSTRAK\\nPenelitian ini bertujuan untuk menget...   \n",
       "4  Penelitian ini bertujuan untuk mengetahui hubu...   \n",
       "\n",
       "                                     abstrak_cleaned  \n",
       "0  Abstrak Tujuan dari penelitian adalah mengetah...  \n",
       "1  Klasifikasi citra merupakan prose pengelompoka...  \n",
       "2  Penelitian berlokasikan Semen Indonesia Pabrik...  \n",
       "3  ABSTRAK Penelitian bertujuan untuk mengetahui ...  \n",
       "4  Penelitian bertujuan untuk mengetahui hubungan...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghapus data yang tidak dibutuhkan\n",
    "hal ini bertujuan agar menggunakan data yang telah di cleansing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['abstrak', 'abstrak_remove'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "S-Ovam3L25aO",
    "outputId": "31dae61f-bd9e-4f68-df28-e922c92176c1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstrak_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abstrak Tujuan dari penelitian adalah mengetah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Klasifikasi citra merupakan prose pengelompoka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Penelitian berlokasikan Semen Indonesia Pabrik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABSTRAK Penelitian bertujuan untuk mengetahui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Penelitian bertujuan untuk mengetahui hubungan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     abstrak_cleaned\n",
       "0  Abstrak Tujuan dari penelitian adalah mengetah...\n",
       "1  Klasifikasi citra merupakan prose pengelompoka...\n",
       "2  Penelitian berlokasikan Semen Indonesia Pabrik...\n",
       "3  ABSTRAK Penelitian bertujuan untuk mengetahui ...\n",
       "4  Penelitian bertujuan untuk mengetahui hubungan..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "wdRYUfvt25aP",
    "outputId": "2d965a50-7cfd-4220-f266-14e1fae9a8b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abstrak Tujuan dari penelitian adalah mengetahui fenomena yang dialami Kelompok Dukungan Sebaya pasien ODHA Orang dengan HIV/AIDS dalam pengembalian Self Esteem Pengalaman selama menjalani kehidupan sosial dengan HIV/AIDS Penelitian menggunakan metode kualitatif fenomenologi Informan dalam penelitian adalah orang pasien relawan dokter konsultan HIV/AIDS Metode pengumpulan data menggunakan observasi wawancara dokumen Objek penelitian adalah Self-esteem ODHA sebelum sesudah relawan Kelompok Dukungan Sebaya memberikan edukasi menunakan komunikasi terapeutik Teknik analisis data menggunakan metode Interpretative Phenomenology Analysis Tujuan penggunaan metode penelitian kualitatif dapat memberikan gambaran mengenai pengalaman seseorang yang hidup dengan HIV/AIDS Hasil penelitian menunjukkan bahwa pasien ODHA yang ketika awal diberitahukan status HIV/AIDS oleh dokter akan mengalami self-esteem Penurunan tersebut disebabkan berbagai faktor yaitu stigma diskriminasi penolakan diri penolakan keluarga Adanya berguna untuk menguatkan diri ODHA mengedukasi ODHA atau keluarga ODHA serta membantu tenaga kesehatan dalam berbagai sosialisasi mengenai HIV/AIDS ODHA yang telah diberikan terapi akan mulai menerima diri percaya diri memberdayakan diri Kata Kunci Self-esteem ODHA HIV/AIDS Fenomenologi'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstrak_cleaned'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MENGEKSTRAK FITUR DAN MEMBUAT DOCUMENT-TERM-MATRIX ( DTM )\n",
    "Dalam DTM nilainya adalah nilai TFidf. Term Frequency — Inverse Document Frequency atau TFIDF adalah suatu metode algoritma yang berguna untuk menghitung bobot setiap kata yang umum digunakan. Metode ini juga terkenal efisien, mudah dan memiliki hasil yang akurat. Secara sederhana, metode TF-IDF digunakan untuk mengetahui berapa sering suatu kata muncul di dalam dokumen. Contoh yang dibahas kali ini adalah mengenai penentuan urutan peringkat data berdasarkan query yang digunakan.\n",
    "\n",
    "Inti utama dari algoritma ini adalah melakukan perhitungan nilai TF dan nilai IDF dari setiap kata kunci terhadap masing-masing dokumen dalam korpus. \n",
    "\n",
    "Term Frequency (TF) yaitu pembobotan/weight setiap kata (term) pada suatu dokumen berdasarkan jumlah kemunculannya dalam dokumen tersebut. Semakin besar jumlah kemunculan suatu kata dalam dokumen, maka semakin besar pula bobot yang diberikan (TF Tinggi) jadi nilai tertinggi merupakan  jumlah kemunculan/frekuensi.\n",
    "\n",
    "Setelah menentukan Tf maka selanjutnya kita tentukan nilai IDF nya dapat dihitung dengan rumus :\n",
    "$$\n",
    "\\operatorname{idf}=\\log \\left(\\frac{D}{d f}\\right)\n",
    "$$\n",
    "\n",
    "Selanjutnya adalah melakukan perkalian antara nilai TF dan IDF untuk mendapatkan jawaban akhir. untuk rumusnya sebagai berikut:\n",
    "$$\n",
    "\\begin{gathered}\n",
    "Tf-Idf=t f_{i j} * i d f_{j} \\\\\n",
    "Tf-Idf=t f_{i j} * \\log \\left(\\frac{D}{d f}\\right)\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "Keterangan :\n",
    "Dimana D adalah jumlah semua dokumen dalam koleksi sedangkan df adalah jumlah dokumen yang mengandung term tertentu.\n",
    "\n",
    "Parameter dari vectorizer Tfidf mmiliki beberapa poin penting:\n",
    "1) LSA umumnya diimplementasikan dengan nilai Tfidf di mana-mana dan tidak dengan Count Vectorizer.\n",
    "\n",
    "2) max_features tergantung pada daya komputasi Anda dan juga pada eval. metrik (skor koherensi adalah metrik untuk model topik). Coba nilai yang memberikan evaluasi terbaik. metrik dan tidak membatasi kekuatan pemrosesan.\n",
    "\n",
    "3) Nilai default untuk min_df & max_df bekerja dengan baik.\n",
    "\n",
    "4) Dapat mencoba nilai yang berbeda untuk ngram_range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'abstrak': 0, 'tujuan': 811, 'dari': 120, 'penelitian': 559, 'adalah': 3, 'mengetahui': 455, 'fenomena': 200, 'yang': 842, 'dialami': 134, 'kelompok': 297, 'dukungan': 181, 'sebaya': 682, 'pasien': 535, 'odha': 514, 'orang': 519, 'dengan': 126, 'hiv': 229, 'aids': 9, 'dalam': 115, 'pengembalian': 570, 'self': 701, 'esteem': 188, 'pengalaman': 560, 'selama': 700, 'menjalani': 474, 'kehidupan': 293, 'sosial': 740, 'menggunakan': 460, 'metode': 492, 'kualitatif': 346, 'fenomenologi': 201, 'informan': 242, 'relawan': 651, 'dokter': 175, 'konsultan': 339, 'pengumpulan': 577, 'data': 122, 'observasi': 512, 'wawancara': 835, 'dokumen': 176, 'objek': 511, 'sebelum': 683, 'sesudah': 716, 'memberikan': 422, 'edukasi': 183, 'menunakan': 478, 'komunikasi': 333, 'terapeutik': 772, 'teknik': 764, 'analisis': 23, 'interpretative': 248, 'phenomenology': 617, 'analysis': 26, 'penggunaan': 572, 'dapat': 119, 'gambaran': 207, 'mengenai': 454, 'seseorang': 713, 'hidup': 226, 'hasil': 223, 'menunjukkan': 480, 'bahwa': 44, 'ketika': 319, 'awal': 38, 'diberitahukan': 142, 'status': 746, 'oleh': 516, 'akan': 12, 'mengalami': 444, 'penurunan': 586, 'tersebut': 785, 'disebabkan': 166, 'berbagai': 64, 'faktor': 190, 'yaitu': 840, 'stigma': 747, 'diskriminasi': 170, 'penolakan': 582, 'diri': 164, 'keluarga': 299, 'adanya': 4, 'berguna': 69, 'untuk': 823, 'menguatkan': 468, 'mengedukasi': 450, 'atau': 36, 'serta': 712, 'membantu': 420, 'tenaga': 768, 'kesehatan': 311, 'sosialisasi': 741, 'telah': 766, 'diberikan': 141, 'terapi': 773, 'mulai': 501, 'menerima': 442, 'percaya': 594, 'memberdayakan': 421, 'kata': 278, 'kunci': 353, 'klasifikasi': 328, 'citra': 108, 'merupakan': 487, 'prose': 635, 'pengelompokan': 568, 'piksel': 619, 'kelas': 294, 'tertentu': 788, 'memiliki': 427, 'batasan': 52, 'sama': 668, 'setiap': 718, 'label': 356, 'fitur': 203, 'khas': 323, 'dijadikan': 150, 'perwakilan': 615, 'kelasnya': 295, 'penting': 583, 'mempermudah': 432, 'pelabelan': 539, 'pada': 523, 'sampah': 669, 'pemberlakuan': 550, 'dilakukan': 154, 'tingkat': 799, 'kompleksitas': 332, 'kepadatan': 301, 'proses': 636, 'feature': 197, 'learning': 368, 'convolutional': 112, 'neural': 508, 'network': 507, 'dikombinasikan': 152, 'nilai': 510, 'tambahan': 761, 'handcrafted': 217, 'features': 198, 'berupa': 86, 'structure': 749, 'diekstrak': 146, 'deteksi': 132, 'tepi': 771, 'operator': 518, 'lapacian': 363, 'diversity': 174, 'sudut': 754, 'algoritma': 18, 'fast': 196, 'accelerated': 2, 'segment': 690, 'test': 790, 'digabungkan': 147, 'fully': 205, 'connected': 111, 'layer': 365, 'sehingga': 691, 'didapatkan': 145, 'keluaran': 298, 'dataset': 123, 'diambil': 135, 'google': 210, 'image': 236, 'sebanyak': 681, 'dibagi': 139, 'menjadi': 473, 'besar': 87, 'training': 805, 'terdapat': 776, 'skenario': 732, 'coba': 110, 'pengaruh': 564, 'jumlah': 264, 'menghasilkan': 463, 'rata': 646, 'score': 679, 'tertinggi': 789, 'sebesar': 684, 'asli': 33, 'augmentasi': 37, 'model': 498, 'peningkatan': 579, 'akurasi': 16, 'kombinasi': 331, 'terhadap': 778, 'disimpulkan': 168, 'semakin': 702, 'banyak': 49, 'digunakan': 148, 'maka': 386, 'kinerja': 326, 'sistem': 729, 'baik': 45, 'berlokasikan': 77, 'semen': 703, 'indonesia': 241, 'pabrik': 521, 'tuban': 809, 'unit': 821, 'kerja': 307, 'packer': 522, 'beberapa': 55, 'jenis': 260, 'kerusakan': 308, 'mesin': 488, 'dapa': 118, 'mengganggu': 459, 'operasional': 517, 'bahkan': 43, 'mematikan': 417, 'produksi': 631, 'pengantongan': 563, 'satu': 677, 'lini': 377, 'terjadi': 779, 'semua': 705, 'pengemasan': 569, 'pengisian': 573, 'hingga': 227, 'akhir': 13, 'memprioritaskan': 435, 'risiko': 659, 'jalannya': 255, 'fuzzy': 206, 'fmea': 204, 'topsis': 803, 'hasilnya': 224, 'diketahui': 151, 'terbesar': 774, 'berhentinya': 70, 'spout': 743, 'mati': 403, 'sedangakan': 688, 'pengolahan': 574, 'reject': 650, 'baut': 53, 'soft': 737, 'jurnal': 265, 'putus': 643, 'bertujuan': 85, 'perhitungan': 597, 'hunian': 234, 'kamar': 268, 'hotel': 231, 'panglima': 531, 'sampang': 671, 'cost': 113, 'volume': 831, 'profit': 632, 'tahun': 760, 'menentukan': 441, 'break': 95, 'event': 189, 'point': 622, 'margin': 396, 'safety': 664, 'bata': 51, 'aman': 20, 'menurunkan': 481, 'penjualan': 580, 'termasuk': 783, 'deskriptif': 131, 'berasal': 63, 'studi': 750, 'pustaka': 642, 'obyek': 513, 'memprediksi': 434, 'hubungan': 233, 'prokrastinasi': 634, 'akademik': 11, 'burnout': 102, 'perilaku': 598, 'plagiarisme': 621, 'mahasiswa': 385, 'psikologi': 637, 'subjek': 752, 'masih': 398, 'aktif': 14, 'kuliah': 351, 'sampel': 672, 'insidental': 245, 'sampling': 673, 'skala': 731, 'likert': 373, 'korelasi': 344, 'product': 629, 'moment': 499, 'bantuan': 48, 'program': 633, 'spss': 744, 'window': 839, 'berdasarkan': 67, 'diperoleh': 161, 'hipotesis': 228, 'diterima': 171, 'antara': 30, 'dilihat': 155, 'signifikansi': 726, 'koefisien': 330, 'tinggi': 798, 'pula': 639, 'kedua': 292, 'kualitas': 345, 'produk': 630, 'harga': 219, 'keputusan': 304, 'pembelian': 547, 'secara': 687, 'parsial': 533, 'maupun': 404, 'simultan': 728, 'perspektif': 609, 'ekonomi': 186, 'islam': 252, 'method': 491, 'dimana': 156, 'menggabungkan': 457, 'kuantitatif': 348, 'bersifat': 83, 'sumber': 757, 'primer': 627, 'sekunder': 699, 'variabel': 829, 'terdiri': 777, 'dependent': 127, 'independent': 238, 'kuesioner': 350, 'kemudian': 300, 'regresi': 649, 'linier': 378, 'berganda': 68, 'keseluruhan': 312, 'berpengaruh': 79, 'signifikan': 725, 'sedangkan': 689, 'tidak': 794, 'literasi': 379, 'zakat': 844, 'pengetahuan': 571, 'keterampilan': 317, 'keyakinan': 322, 'mempengaruhi': 430, 'sikap': 727, 'meningkatkan': 472, 'pengambilan': 561, 'suatu': 751, 'penyaluran': 590, 'melalui': 410, 'lembaga': 370, 'amil': 21, 'analitis': 25, 'masyarakat': 400, 'derah': 129, 'perkotaan': 600, 'sekitar': 696, 'lazismu': 366, 'pamekasan': 529, 'semiterstruktur': 704, 'agar': 7, 'menjawab': 475, 'pertanyaan': 610, 'disiapkan': 167, 'menggali': 458, 'informasi': 243, 'nasabah': 505, 'tahapan': 759, 'dimulai': 157, 'sejak': 692, 'reduksi': 648, 'penyajian': 589, 'penarikan': 554, 'kesimpulan': 313, 'tingkatan': 800, 'berbeda': 66, 'beda': 56, 'karena': 275, 'kategori': 279, 'well': 836, 'literate': 380, 'sufficient': 755, 'berjumlah': 72, 'le': 367, 'paling': 528, 'diantara': 137, 'lainnya': 359, 'keuangan': 321, 'pengelola': 566, 'febrianti': 199, 'sosiologi': 742, 'fakultas': 192, 'ilmu': 235, 'budaya': 97, 'universitas': 822, 'trunojoyo': 807, 'madura': 383, 'jaringan': 256, 'pengusaha': 578, 'kerajinan': 305, 'limbah': 375, 'kayu': 281, 'jati': 258, 'dosen': 179, 'pembimbing': 551, 'khoirul': 324, 'rosyadi': 660, 'ph': 616, 'pengrajin': 575, 'pembeli': 546, 'dokumentasi': 177, 'dipilih': 162, 'purposive': 641, 'miles': 494, 'huberman': 232, 'pemeriksaan': 552, 'keabsahan': 282, 'triangulasi': 806, 'menganalisis': 445, 'teori': 770, 'marc': 393, 'granovetter': 211, 'tentang': 769, 'didalam': 144, 'desa': 130, 'setempat': 717, 'disamping': 165, 'juga': 263, 'bentuk': 60, 'diantaranya': 138, 'rasa': 645, 'persaudaraan': 607, 'kuat': 349, 'persahabatan': 606, 'kesamaan': 310, 'tempat': 767, 'tinggal': 797, 'strategi': 748, 'pemasaran': 543, 'memasarkan': 416, 'barang': 50, 'mengikuti': 465, 'pameran': 530, 'menyebar': 482, 'kartu': 276, 'nama': 504, 'kerajinannya': 306, 'shorum': 721, 'miliknya': 495, 'sendiri': 706, 'medium': 407, 'whatsapp': 837, 'perusahaan': 614, 'bagian': 42, 'terpenting': 784, 'investor': 250, 'calon': 104, 'menanamkan': 437, 'dananya': 117, 'berkembangnya': 74, 'dunia': 182, 'investasi': 249, 'saat': 663, 'membuat': 424, 'terdaftar': 775, 'jakarta': 254, 'islamic': 253, 'index': 239, 'perlu': 602, 'guna': 214, 'memperoleh': 433, 'sejumlah': 694, 'dana': 116, 'mengembangkan': 453, 'usahanya': 825, 'adapun': 5, 'indikator': 240, 'sering': 711, 'menilai': 471, 'asset': 34, 'growth': 212, 'current': 114, 'ratio': 647, 'debt': 125, 'equity': 187, 'peneliti': 558, 'tertarik': 787, 'meliputi': 412, 'laporan': 364, 'kapitalisasi': 272, 'pasar': 534, 'kurs': 354, 'dolar': 178, 'penutupan': 587, 'populasi': 623, 'konsisten': 337, 'asumsi': 35, 'klasik': 329, 'linear': 376, 'determinan': 133, 'presentase': 626, 'dipengaruhi': 160, 'lain': 358, 'negatif': 506, 'positif': 624, 'farida': 193, 'yulistiana': 843, 'pertukaran': 612, 'tradisi': 804, 'sape': 675, 'tok': 801, 'ingin': 244, 'mana': 390, 'perayaan': 592, 'pernikahan': 604, 'bringen': 96, 'kecamatan': 285, 'labang': 355, 'kabupaten': 267, 'bangkalan': 47, 'budayawan': 98, 'pelaku': 540, 'marcell': 394, 'mauss': 405, 'pemberian': 549, 'seserahan': 714, 'berlangsung': 75, 'pihak': 618, 'pengantin': 562, 'laki': 360, 'sebuah': 685, 'menyebutnya': 483, 'manten': 391, 'bukan': 99, 'hanya': 218, 'saja': 665, 'berikan': 71, 'tetapi': 792, 'harus': 222, 'lengkap': 371, 'mewah': 493, 'membutuhkan': 425, 'siang': 722, 'hari': 221, 'bersamaan': 82, 'iringan': 251, 'perempuan': 596, 'membalasnya': 418, 'balasan': 46, 'setimpal': 719, 'malam': 388, 'hiburan': 225, 'bisa': 92, 'mengundang': 469, 'menghadiri': 462, 'memeriahkan': 426, 'resepsi': 653, 'pernikahannya': 605, 'mempunyai': 436, 'makna': 387, 'tersendiri': 786, 'bagi': 41, 'mengangkat': 446, 'drajat': 180, 'masing': 399, 'sebut': 686, 'mampu': 389, 'sangat': 674, 'rumah': 662, 'sakit': 666, 'salah': 667, 'instansi': 246, 'bidang': 89, 'pelayanan': 541, 'jasa': 257, 'beroprasi': 78, 'penuh': 584, 'perawat': 591, 'memadai': 413, 'pekerjaan': 538, 'seorang': 708, 'lepas': 372, 'shift': 720, 'rsud': 661, 'soetomo': 736, 'menggunkan': 461, 'tiga': 795, 'pembagian': 544, 'pagi': 526, 'sore': 739, 'mengevaluasi': 456, 'kelelahan': 296, 'bourdon': 94, 'wiersma': 838, 'penlitian': 581, 'bekerja': 57, 'mengindikasikan': 466, 'value': 828, 'kecepatan': 287, 'ketelitian': 315, 'konsistensi': 338, 'dibandingkan': 140, 'solusi': 738, 'usulan': 827, 'meminimasi': 428, 'lantai': 362, 'melakukan': 409, 'pengaturan': 565, 'usia': 826, 'sesuai': 715, 'penambahan': 553, 'daya': 124, 'ketenagakerjaan': 316, 'keperawatan': 302, 'mengurangi': 470, 'beban': 54, 'pendapatan': 556, 'bopo': 93, 'loan': 381, 'deposit': 128, 'return': 657, 'capital': 105, 'adequacy': 6, 'sektor': 698, 'perbankan': 593, 'bursa': 103, 'efek': 184, 'periode': 599, 'pendekatan': 557, 'melihat': 411, 'berapa': 62, 'kontribusi': 340, 'independen': 237, 'pengujian': 576, 'kepercayaan': 303, 'mennunjukkan': 476, 'gerakan': 208, 'umumnya': 820, 'permasalahan': 603, 'ketersediaan': 318, 'pengelolaan': 567, 'sumberdaya': 758, 'berkaitan': 73, 'skripsi': 733, 'bagaimana': 40, 'praktik': 625, 'mobilisasi': 496, 'baca': 39, 'tulis': 812, 'serikat': 710, 'peduli': 536, 'anggota': 28, 'karakteristik': 273, 'sudah': 753, 'ditetapkan': 172, 'resource': 654, 'mobilization': 497, 'theory': 793, 'sebagai': 680, 'landasan': 361, 'analisisnya': 24, 'agregasi': 8, 'kooptasi': 342, 'mekanisme': 408, 'ak': 10, 'aktor': 15, 'tulisnya': 814, 'lima': 374, 'yakni': 841, 'moral': 500, 'menunjukan': 479, 'loyalitas': 382, 'berlebih': 76, 'kultural': 352, 'ngopi': 509, 'organisasi': 520, 'pemanfaatan': 542, 'fasilitas': 195, 'publik': 638, 'komunitas': 334, 'manusia': 392, 'kedekatan': 289, 'personal': 608, 'material': 402, 'uang': 818, 'warung': 834, 'kopi': 343, 'buku': 100, 'bertahan': 84, 'pemberdayaan': 548, 'kampanye': 269, 'bekerjasama': 58, 'membangun': 419, 'berbasis': 65, 'membingkai': 423, 'musik': 503, 'penyadaran': 588, 'tulisan': 813, 'respon': 655, 'siswa': 730, 'rendah': 652, 'pembelajaran': 545, 'daring': 121, 'dihadapi': 149, 'upaya': 824, 'sekolah': 697, 'guru': 215, 'mengatasi': 447, 'tunjung': 816, 'kuantatif': 347, 'responden': 656, 'angket': 29, 'tunggal': 815, 'problematika': 628, 'kebanyakan': 283, 'berpenghasilan': 80, 'ribu': 658, 'juta': 266, 'bulan': 101, 'anggapan': 27, 'biaya': 88, 'paket': 527, 'internet': 247, 'mahal': 384, 'apalagi': 31, 'alat': 17, 'teknologi': 765, 'menemani': 440, 'belajar': 59, 'kecuali': 288, 'pegawai': 537, 'merasa': 486, 'jenuh': 261, 'kesulitan': 314, 'memahami': 414, 'mengeluhkan': 452, 'materi': 401, 'dinilai': 158, 'mencatat': 439, 'dibuku': 143, 'catatan': 106, 'aplikasi': 32, 'variatif': 830, 'ceramah': 107, 'menyuruh': 485, 'menonton': 477, 'tvri': 817, 'fasih': 194, 'grub': 213, 'waktu': 832, 'satunya': 678, 'signal': 724, 'stabil': 745, 'anak': 22, 'mengawasi': 449, 'sulit': 756, 'dipahami': 159, 'jika': 262, 'soal': 734, 'terlalu': 780, 'mengeluh': 451, 'kondisinya': 336, 'penunjang': 585, 'masalah': 397, 'mencapai': 438, 'kkm': 327, 'menyediakan': 484, 'meskipun': 489, 'harganya': 220, 'sejauh': 693, 'terlihat': 782, 'berani': 61, 'meminta': 429, 'disiplin': 169, 'memang': 415, 'ditingkatkan': 173, 'lagi': 357, 'kesadaran': 309, 'sendri': 707, 'alternatif': 19, 'tugas': 810, 'karya': 277, 'menghubungi': 464, 'tetap': 791, 'dianjurkan': 136, 'mengatur': 448, 'pertemuan': 611, 'mengajarkan': 443, 'seperti': 709, 'classroom': 109, 'tua': 808, 'wali': 833, 'murid': 502, 'biochar': 90, 'padatan': 524, 'kaya': 280, 'kandungan': 271, 'karbon': 274, 'konversi': 341, 'biomassa': 91, 'pirolisis': 620, 'pencampuran': 555, 'bersama': 81, 'pupuk': 640, 'kandang': 270, 'efektivitas': 185, 'memperbaiki': 431, 'sifat': 723, 'tanah': 762, 'mengkaji': 467, 'sekam': 695, 'padi': 525, 'sapi': 676, 'fisik': 202, 'kimia': 325, 'mediteran': 406, 'tanaman': 763, 'kedelai': 290, 'glycine': 209, 'dilaksanakan': 153, 'kebun': 284, 'hortikultura': 230, 'socah': 735, 'jawa': 259, 'timur': 796, 'terletak': 781, 'ketinggian': 320, 'meter': 490, 'oktober': 515, 'sampai': 670, 'maret': 395, 'percobaan': 595, 'dirancang': 163, 'rancangan': 644, 'acak': 1, 'faktorial': 191, 'ton': 802, 'ha': 216, 'pertumbuhan': 613, 'parameter': 532, 'umum': 819, 'kecenderungan': 286, 'kondisi': 335, 'kedele': 291, 'lebih': 369, 'perlakuan': 601}\n",
      "Encoded Document is:\n",
      "[[1 0 0 ... 4 0 0]\n",
      " [0 0 1 ... 8 0 0]\n",
      " [0 0 0 ... 7 0 0]\n",
      " ...\n",
      " [0 0 0 ... 3 0 0]\n",
      " [0 0 0 ... 7 0 0]\n",
      " [0 1 0 ... 4 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "document = df['abstrak_cleaned']\n",
    "a=len(document)\n",
    "\n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(document)\n",
    "\n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "\n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(document)\n",
    "\n",
    "# Summarizing the Encoded Texts\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "a = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "tf = tfidf.fit_transform(vectorizer.fit_transform(document)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>abstrak</th>\n",
       "      <th>acak</th>\n",
       "      <th>accelerated</th>\n",
       "      <th>adalah</th>\n",
       "      <th>adanya</th>\n",
       "      <th>adapun</th>\n",
       "      <th>adequacy</th>\n",
       "      <th>agar</th>\n",
       "      <th>agregasi</th>\n",
       "      <th>aids</th>\n",
       "      <th>...</th>\n",
       "      <th>wawancara</th>\n",
       "      <th>well</th>\n",
       "      <th>whatsapp</th>\n",
       "      <th>wiersma</th>\n",
       "      <th>window</th>\n",
       "      <th>yaitu</th>\n",
       "      <th>yakni</th>\n",
       "      <th>yang</th>\n",
       "      <th>yulistiana</th>\n",
       "      <th>zakat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.038396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068568</td>\n",
       "      <td>0.038396</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.382617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069966</td>\n",
       "      <td>0.066798</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.047512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.054544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.140774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034717</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.032185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133911</td>\n",
       "      <td>0.045819</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042630</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.038043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137740</td>\n",
       "      <td>0.077129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106967</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019319</td>\n",
       "      <td>0.032454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040118</td>\n",
       "      <td>0.045009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.081173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030320</td>\n",
       "      <td>0.079372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15 rows × 845 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abstrak      acak accelerated    adalah    adanya    adapun adequacy  \\\n",
       "1   0.038396  0.000000    0.000000  0.068568  0.038396  0.000000   0.0000   \n",
       "2   0.000000  0.000000    0.041401  0.000000  0.029082  0.000000   0.0000   \n",
       "3   0.000000  0.000000    0.000000  0.140922  0.000000  0.000000   0.0000   \n",
       "4   0.046344  0.000000    0.000000  0.000000  0.000000  0.000000   0.0000   \n",
       "5   0.000000  0.000000    0.000000  0.029256  0.000000  0.000000   0.0000   \n",
       "6   0.047512  0.000000    0.000000  0.056566  0.000000  0.000000   0.0000   \n",
       "7   0.000000  0.000000    0.000000  0.088298  0.000000  0.000000   0.0000   \n",
       "8   0.000000  0.000000    0.000000  0.079625  0.000000  0.000000   0.0000   \n",
       "9   0.000000  0.000000    0.000000  0.050155  0.000000  0.034717   0.0000   \n",
       "10  0.032185  0.000000    0.000000  0.057478  0.000000  0.000000   0.0000   \n",
       "11  0.000000  0.000000    0.000000  0.000000  0.000000  0.042630   0.0000   \n",
       "12  0.000000  0.000000    0.000000  0.137740  0.077129  0.000000   0.1098   \n",
       "13  0.000000  0.000000    0.000000  0.019319  0.032454  0.000000   0.0000   \n",
       "14  0.000000  0.000000    0.000000  0.175210  0.000000  0.000000   0.0000   \n",
       "15  0.000000  0.049754    0.000000  0.000000  0.000000  0.000000   0.0000   \n",
       "\n",
       "        agar  agregasi      aids  ... wawancara      well  whatsapp   wiersma  \\\n",
       "1   0.000000  0.000000  0.382617  ...  0.030053  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.000000  ...  0.036275  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  ...  0.037189  0.000000  0.000000  0.000000   \n",
       "7   0.054544  0.000000  0.000000  ...  0.038700  0.140774  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  ...  0.026174  0.000000  0.041337  0.000000   \n",
       "9   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  ...  0.025192  0.000000  0.000000  0.000000   \n",
       "11  0.038043  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.049094   \n",
       "12  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.046201  0.000000  ...  0.025402  0.000000  0.000000  0.000000   \n",
       "14  0.081173  0.000000  0.000000  ...  0.000000  0.000000  0.030320  0.000000   \n",
       "15  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "      window     yaitu     yakni      yang yulistiana     zakat  \n",
       "1   0.000000  0.026092  0.000000  0.070999   0.000000  0.000000  \n",
       "2   0.000000  0.039526  0.000000  0.107555   0.000000  0.000000  \n",
       "3   0.000000  0.000000  0.000000  0.191517   0.000000  0.000000  \n",
       "4   0.000000  0.000000  0.000000  0.021424   0.000000  0.000000  \n",
       "5   0.069966  0.066798  0.000000  0.068161   0.000000  0.000000  \n",
       "6   0.000000  0.129149  0.000000  0.043928   0.000000  0.000000  \n",
       "7   0.000000  0.033600  0.000000  0.182856   0.000000  0.281548  \n",
       "8   0.000000  0.045449  0.000000  0.061836   0.000000  0.000000  \n",
       "9   0.000000  0.000000  0.000000  0.129832   0.000000  0.000000  \n",
       "10  0.000000  0.065616  0.000000  0.133911   0.045819  0.000000  \n",
       "11  0.000000  0.023435  0.000000  0.063770   0.000000  0.000000  \n",
       "12  0.000000  0.000000  0.000000  0.106967   0.000000  0.000000  \n",
       "13  0.000000  0.000000  0.040118  0.045009   0.000000  0.000000  \n",
       "14  0.000000  0.000000  0.030320  0.079372   0.000000  0.000000  \n",
       "15  0.000000  0.023751  0.000000  0.064627   0.000000  0.000000  \n",
       "\n",
       "[15 rows x 845 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfb = pd.DataFrame(data=tf,index=list(range(1, len(tf[:,1])+1, )),columns=[a])\n",
    "dfb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03839557 0.         0.         ... 0.07099933 0.         0.        ]\n",
      " [0.         0.         0.04140108 ... 0.10755476 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.19151678 0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.04500933 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.07937183 0.         0.        ]\n",
      " [0.         0.04975402 0.         ... 0.06462733 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#--- Mengubah Variabel Data Frame Menjadi Array ---\n",
    "x_array =  np.array(dfb)\n",
    "print(x_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA\n",
    "Latent Semantic Analysis (LSA) merupakan sebuah metode yang memanfaatkan model statistik matematis untuk menganalisa struktur semantik suatu teks. LSA bisa digunakan untuk menilai esai dengan mengkonversikan esai menjadi matriks-matriks yang diberi nilai pada masing-masing term untuk dicari kesamaan dengan term referensi. LSA pada dasarnya adalah dekomposisi nilai tunggal.\n",
    "\n",
    "Singular Value Decomposition (SVD) adalah salah satu teknik reduksi dimensi yang bermanfaat untuk memperkecil nilai kompleksitas dalam pemrosesan term-document matrix. SVD merupakan teorema aljabar linier yang menyebutkan bahwa persegi panjang dari term-document matrix dapat dipecah/didekomposisikan menjadi tiga matriks, yaitu :\n",
    "\n",
    "– Matriks ortogonal U (matriks dokumen-topik)\n",
    "\n",
    "– Matriks diagonal S (Matrik diagonal dengan elemen matriks positif atau nol)\n",
    "\n",
    "– Transpose dari matriks ortogonal V (matriks topik-term)\n",
    "\n",
    "Yang dirumuskan dengan :\n",
    "$$\n",
    "A_{m n}=U_{m m} x S_{m n} x V_{n n}^{T}\n",
    "$$\n",
    "\n",
    "Keterangan : \n",
    "A = Matriks Masukan (Pada Penelitian matriks ini berisi matrik hasil perhitungan TF-IDF)\n",
    "\n",
    "U = Matriks Ortogonal U\n",
    "\n",
    "S = Matriks Diagonal S (matriks positif atau nol)\n",
    "\n",
    "V =  Transpose Ortogonal V\n",
    "\n",
    "\n",
    "Setiap baris dari matriks U (matriks istilah dokumen) adalah representasi vektor dari dokumen yang sesuai. Panjang vektor ini adalah jumlah topik yang diinginkan. Representasi vektor untuk suku-suku dalam data kami dapat ditemukan dalam matriks V (matriks istilah-topik).\n",
    "\n",
    "Jadi, SVD memberi kita vektor untuk setiap dokumen dan istilah dalam data kita. Panjang setiap vektor adalah k. Kami kemudian dapat menggunakan vektor-vektor ini untuk menemukan kata-kata dan dokumen serupa menggunakan metode kesamaan kosinus.\n",
    "\n",
    "Kita dapat menggunakan fungsi truncatedSVD untuk mengimplementasikan LSA. Parameter n_components adalah jumlah topik yang ingin kita ekstrak. Model tersebut kemudian di fit dan ditransformasikan pada hasil yang diberikan oleh vectorizer.\n",
    "\n",
    "Terakhir perhatikan bahwa LSA dan LSI (I untuk pengindeksan) adalah sama dan yang terakhir kadang-kadang digunakan dalam konteks pencarian informasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "1YJy_PQJ25aS"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "\n",
    "lsa_top=lsa_model.fit_transform(vect_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirip dengan dokumen lain kita bisa melakukan ini. Namun perhatikan bahwa nilai tidak menambah 1 seperti di LSA itu bukan kemungkinan topik dalam dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "O_JpZBOI25aT",
    "outputId": "a951db88-a602-4806-99c9-90b062073224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28739974 -0.16443604 -0.24485936 -0.33589771 -0.24179411 -0.18664379\n",
      "   0.65178303  0.05227756  0.03566168 -0.31864152]\n",
      " [ 0.33031229 -0.23837623  0.25331581 -0.07973845  0.14389884  0.04690495\n",
      "  -0.17611137 -0.21462627  0.58533165 -0.48285431]\n",
      " [ 0.34029815 -0.06399777  0.0021366  -0.40835494  0.00251463  0.61254533\n",
      "  -0.0299833  -0.32362406  0.03444427  0.29966537]\n",
      " [ 0.31223536 -0.29800223  0.48424529  0.23353168 -0.09186837 -0.09375818\n",
      "   0.14747139  0.17556333 -0.25673856 -0.01868605]\n",
      " [ 0.33379767 -0.26322689  0.14573504  0.19675219 -0.03456838 -0.37474894\n",
      "   0.00570414  0.00265815  0.42589565  0.56554216]\n",
      " [ 0.45001104  0.36064816 -0.07610639  0.10442093 -0.00191951 -0.00457416\n",
      "   0.30146721 -0.10004318 -0.02922915  0.1568398 ]\n",
      " [ 0.41346476 -0.07543995 -0.33483044  0.26348518  0.0510957   0.32533185\n",
      "   0.1754745   0.1580012   0.09325845  0.14502383]\n",
      " [ 0.34913247 -0.19328175 -0.33533609  0.13040218 -0.03354267 -0.27991346\n",
      "  -0.06671823 -0.49351557 -0.1065744  -0.03992579]\n",
      " [ 0.48897212  0.5038293   0.21835419  0.04366816 -0.01235192 -0.07314673\n",
      "  -0.17885665  0.0912498   0.00659425 -0.20565372]\n",
      " [ 0.3592441  -0.20846058 -0.24226467 -0.11460069 -0.01713637 -0.22704645\n",
      "  -0.40128809 -0.18184171 -0.41867508 -0.03466391]\n",
      " [ 0.29351144 -0.37200345  0.49014298 -0.09036889 -0.13217172  0.20265444\n",
      "   0.04161334  0.04477038 -0.3163217  -0.00243471]\n",
      " [ 0.58298982  0.52221544  0.12344467  0.02946768 -0.01901382 -0.02927891\n",
      "  -0.04016258 -0.00733036 -0.06316499 -0.01869101]\n",
      " [ 0.27636547 -0.2447704  -0.33856013  0.48051784  0.10575461  0.30271722\n",
      "  -0.12925523  0.28802317 -0.01461163 -0.23666051]\n",
      " [ 0.29949486 -0.06861005 -0.22610162 -0.4657087  -0.2328387  -0.11058737\n",
      "  -0.32171392  0.56035384  0.12341356  0.10263734]\n",
      " [ 0.17243082 -0.07812189  0.00933528 -0.24574209  0.90098614 -0.15242915\n",
      "   0.13379487  0.15487643 -0.12516753  0.05125763]]\n",
      "(15, 10)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_top)\n",
    "print(lsa_top.shape)  # (no_of_doc*no_of_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 :\n",
      "Topic  0  :  28.73997402628853\n",
      "Topic  1  :  -16.44360412433165\n",
      "Topic  2  :  -24.485935841557176\n",
      "Topic  3  :  -33.58977110950959\n",
      "Topic  4  :  -24.179410857984777\n",
      "Topic  5  :  -18.664379071085957\n",
      "Topic  6  :  65.17830259289043\n",
      "Topic  7  :  5.227755775427135\n",
      "Topic  8  :  3.5661678168832243\n",
      "Topic  9  :  -31.864152439230086\n"
     ]
    }
   ],
   "source": [
    "l=lsa_top[0]\n",
    "print(\"Document 0 :\")\n",
    "for i,topic in enumerate(l):\n",
    "  print(\"Topic \",i,\" : \",topic*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 843)\n",
      "[[ 0.02919851  0.00428579  0.00679985 ...  0.25345966  0.00822286\n",
      "   0.05815389]\n",
      " [-0.00809377 -0.0032437  -0.00819764 ... -0.02967318 -0.00797092\n",
      "  -0.01772528]\n",
      " [ 0.00142989  0.00040818  0.00917372 ... -0.04136295 -0.00975511\n",
      "  -0.08284649]\n",
      " ...\n",
      " [-0.00049859  0.00831002 -0.009538   ... -0.02836817 -0.00898518\n",
      "   0.04797344]\n",
      " [-0.02748833 -0.00674145  0.0261109  ...  0.0253416  -0.02076613\n",
      "   0.02842326]\n",
      " [-0.00747515  0.00281825 -0.02198852 ...  0.02151943 -0.00175516\n",
      "   0.04512167]]\n"
     ]
    }
   ],
   "source": [
    "print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n",
    "print(lsa_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekarang bisa mendapatkan daftar kata-kata penting dari masing-masing untuk 10 topik seperti yang ditunjukkan. Untuk kesederhanaan di sini saya telah menunjukkan 10 kata untuk setiap topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "VfpEJrck25aT",
    "outputId": "3b389775-93e9-4d22-cedb-730fca14ad68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "yang penelitian perusahaan adalah dengan data terhadap ratio dalam bopo \n",
      "\n",
      "Topic 1: \n",
      "perusahaan ratio bopo berpengaruh signifikan terhadap secara pembelian asset keputusan \n",
      "\n",
      "Topic 2: \n",
      "tingkat shift sebesar kamar kerja hunian tahun perusahaan nilai perawat \n",
      "\n",
      "Topic 3: \n",
      "literasi sumberdaya literate kamar gerakan mobilisasi peduli serikat data mahasiswa \n",
      "\n",
      "Topic 4: \n",
      "biochar kandang pupuk tanah sifat fisik kedelai kimia memperbaiki padi \n",
      "\n",
      "Topic 5: \n",
      "mesin semen literasi literate kerusakan kerja pengemasan sumberdaya shift prose \n",
      "\n",
      "Topic 6: \n",
      "odha aids hiv diri esteem self pembelian keputusan harga produk \n",
      "\n",
      "Topic 7: \n",
      "siswa pembelajaran literasi sumberdaya orang permasalahan daring rendah upaya kamar \n",
      "\n",
      "Topic 8: \n",
      "model fitur mahasiswa dataset burnout prokrastinasi akademik citra coba klasifikasi \n",
      "\n",
      "Topic 9: \n",
      "mahasiswa akademik burnout prokrastinasi psikologi skala mesin semen kerusakan literate \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# most important words for each topic\n",
    "vocab = vect.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXBo7CO025aX"
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc30f45a0ee3bb283605bb62d26dc81a10ff7c6c6d220b9edda588a614f8ccf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}